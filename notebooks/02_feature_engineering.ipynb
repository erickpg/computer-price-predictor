{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering: Computer Price Prediction\n",
    "\n",
    "## Simplified and Improved Pipeline\n",
    "\n",
    "This notebook applies **simplified feature engineering** with:\n",
    "\n",
    "1. **Data Quality Analysis** - Detect format issues, mixed types, and column groupings\n",
    "2. **CPU/GPU Parsing** - Extract normalized keys (brand, family, model, suffix)\n",
    "3. **Benchmark Matching** - Exact matching first, then fuzzy matching with scores\n",
    "4. **Feature Extraction** - 18 engineered features for modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Reload features module\n",
    "sys.path.append('..')\n",
    "for mod in ['src.features', 'features']:\n",
    "    if mod in sys.modules:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "from src.features import (\n",
    "    cargar_datos, construir_features,\n",
    "    analyze_format_issues, print_column_groups,\n",
    "    parse_cpu_name, parse_gpu_name\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_theme(style='whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data')\n",
    "\n",
    "df_comp, df_cpu, df_gpu = cargar_datos(\n",
    "    str(DATA_DIR / 'db_computers_2025_raw.csv'),\n",
    "    str(DATA_DIR / 'db_cpu_raw.csv'),\n",
    "    str(DATA_DIR / 'db_gpu_raw.csv')\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  Computers: {df_comp.shape}\")\n",
    "print(f\"  CPU benchmarks: {df_cpu.shape}\")\n",
    "print(f\"  GPU benchmarks: {df_gpu.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Quality Analysis\n",
    "\n",
    "Detect format issues, mixed types, and column groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze format issues in each dataset\n",
    "df_comp_issues = analyze_format_issues(df_comp, \"Computers dataset\")\n",
    "df_cpu_issues = analyze_format_issues(df_cpu, \"CPU benchmarks\")\n",
    "df_gpu_issues = analyze_format_issues(df_gpu, \"GPU benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns with format issues\n",
    "print(\"\\n=== Computers: Columns with mixed types ===\")\n",
    "mixed_cols = df_comp_issues[df_comp_issues['mixed_numeric_text'] == True]\n",
    "if len(mixed_cols) > 0:\n",
    "    display(mixed_cols[['column', 'sample_text_values']].head(10))\n",
    "else:\n",
    "    print(\"No mixed type columns detected\")\n",
    "\n",
    "print(\"\\n=== Computers: Columns with multilabel values ===\")\n",
    "multilabel = df_comp_issues[df_comp_issues['multilabel_rows'] > 100].sort_values('multilabel_rows', ascending=False)\n",
    "if len(multilabel) > 0:\n",
    "    display(multilabel[['column', 'multilabel_rows']].head(10))\n",
    "else:\n",
    "    print(\"No significant multilabel columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column groups by prefix\n",
    "print_column_groups(df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. CPU/GPU Parsing Preview\n",
    "\n",
    "Test the parsing logic on sample data before running full feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CPU parsing on sample data\n",
    "print(\"=== CPU Parsing Examples ===\")\n",
    "cpu_samples = df_comp['Procesador_Procesador'].dropna().sample(10, random_state=42)\n",
    "\n",
    "for cpu in cpu_samples:\n",
    "    parsed = parse_cpu_name(cpu)\n",
    "    print(f\"\\nOriginal: {cpu}\")\n",
    "    print(f\"  -> Key: {parsed['cpu_normalized_key']}\")\n",
    "    print(f\"  -> Brand: {parsed['cpu_brand']}, Family: {parsed['cpu_family']}\")\n",
    "    print(f\"  -> Model: {parsed['cpu_model_code']}, Suffix: {parsed['cpu_suffix']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU parsing on sample data\n",
    "print(\"=== GPU Parsing Examples ===\")\n",
    "gpu_samples = df_comp['Gr치fica_Tarjeta gr치fica'].dropna().sample(10, random_state=21)\n",
    "\n",
    "for gpu in gpu_samples:\n",
    "    parsed = parse_gpu_name(gpu)\n",
    "    print(f\"\\nOriginal: {gpu}\")\n",
    "    print(f\"  -> Key: {parsed['gpu_normalized_key']}\")\n",
    "    print(f\"  -> Brand: {parsed['gpu_brand']}, Series: {parsed['gpu_series']}\")\n",
    "    print(f\"  -> Model: {parsed['gpu_model_number']}, Integrated: {parsed['gpu_is_integrated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Run Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all engineered features\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProcessing {len(df_comp):,} computer listings...\\n\")\n",
    "\n",
    "df_feat = construir_features(df_comp, df_cpu, df_gpu)\n",
    "\n",
    "print(f\"\\nDataframe shape: {df_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all engineered features\n",
    "engineered = sorted([c for c in df_feat.columns if c.startswith('_')])\n",
    "print(f\"\\nTotal engineered features: {len(engineered)}\\n\")\n",
    "\n",
    "for i, feat in enumerate(engineered, 1):\n",
    "    non_null = df_feat[feat].notna().sum()\n",
    "    pct = non_null / len(df_feat) * 100\n",
    "    print(f\"{i:2d}. {feat:35s}: {non_null:5,}/{len(df_feat):,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. CPU/GPU Matching Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU matching summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CPU BENCHMARK MATCHING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nMatch strategy distribution:\")\n",
    "print(df_feat['cpu_match_strategy'].value_counts(dropna=False))\n",
    "\n",
    "# Coverage by brand\n",
    "print(\"\\nCoverage by CPU brand:\")\n",
    "for brand in ['intel', 'amd', 'apple', 'qualcomm']:\n",
    "    mask = df_feat['cpu_brand'] == brand\n",
    "    if mask.sum() > 0:\n",
    "        matched = df_feat.loc[mask, 'cpu_bench_mark'].notna().sum()\n",
    "        total = mask.sum()\n",
    "        print(f\"  {brand.capitalize():10s}: {matched:4,}/{total:4,} ({matched/total*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU matching summary\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU BENCHMARK MATCHING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nMatch strategy distribution:\")\n",
    "print(df_feat['gpu_match_strategy'].value_counts(dropna=False))\n",
    "\n",
    "# Show discrete GPU coverage\n",
    "discrete_mask = df_feat['gpu_is_integrated'] != True\n",
    "if discrete_mask.sum() > 0:\n",
    "    matched = df_feat.loc[discrete_mask, 'gpu_bench_mark'].notna().sum()\n",
    "    total = discrete_mask.sum()\n",
    "    print(f\"\\nDiscrete GPU coverage: {matched:,}/{total:,} ({matched/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample matched CPUs\n",
    "print(\"\\n=== Sample Matched CPUs ===\")\n",
    "matched_cpus = df_feat[df_feat['cpu_match_strategy'].isin(['exact', 'fuzzy'])][\n",
    "    ['Procesador_Procesador', 'cpu_normalized_key', 'cpu_bench_name', \n",
    "     'cpu_bench_mark', 'cpu_match_strategy', 'cpu_match_score']\n",
    "].head(15)\n",
    "display(matched_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample matched GPUs\n",
    "print(\"\\n=== Sample Matched GPUs ===\")\n",
    "matched_gpus = df_feat[df_feat['gpu_match_strategy'].isin(['exact', 'fuzzy'])][\n",
    "    ['Gr치fica_Tarjeta gr치fica', 'gpu_normalized_key', 'gpu_bench_name', \n",
    "     'gpu_bench_mark', 'gpu_match_strategy', 'gpu_match_score']\n",
    "].head(15)\n",
    "display(matched_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with price\n",
    "numeric_feats = [\n",
    "    '_ram_gb', '_ssd_gb', '_cpu_cores', '_gpu_memory_gb',\n",
    "    '_cpu_mark', '_gpu_mark', '_tamano_pantalla_pulgadas',\n",
    "    '_resolucion_pixeles', '_tasa_refresco_hz', '_peso_kg',\n",
    "    '_num_ofertas', '_precio_num'\n",
    "]\n",
    "\n",
    "available = [f for f in numeric_feats if f in df_feat.columns]\n",
    "corr = df_feat[available].corr()['_precio_num'].drop('_precio_num').sort_values(ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION WITH PRICE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Feature':<35s} {'Correlation':>12s} {'Strength':>12s}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for feat, c in corr.items():\n",
    "    if pd.notna(c):\n",
    "        strength = \"Strong\" if abs(c) >= 0.5 else \"Moderate\" if abs(c) >= 0.3 else \"Weak\"\n",
    "        print(f\"{feat:<35s} {c:>12.3f} {strength:>12s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "corr.plot(kind='barh', ax=ax, color=['green' if x > 0 else 'red' for x in corr])\n",
    "ax.set_xlabel('Correlation with Price')\n",
    "ax.set_title('Feature Correlations with Price')\n",
    "ax.axvline(0.3, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.axvline(0.5, color='red', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 8. Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of key features\n",
    "key_feats = ['_precio_num', '_ram_gb', '_ssd_gb', '_cpu_cores', '_cpu_mark', '_gpu_mark']\n",
    "key_feats = [f for f in key_feats if f in df_feat.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feat in enumerate(key_feats):\n",
    "    if idx < len(axes):\n",
    "        data = df_feat[feat].dropna()\n",
    "        if len(data) > 0:\n",
    "            axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
    "            axes[idx].set_xlabel(feat)\n",
    "            axes[idx].set_title(f'Distribution of {feat}')\n",
    "            axes[idx].axvline(data.median(), color='red', linestyle='--', \n",
    "                             label=f'Median: {data.median():.1f}')\n",
    "            axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 9. Missing Values Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values for engineered features\n",
    "eng_feats = [c for c in df_feat.columns if c.startswith('_')]\n",
    "missing = pd.DataFrame({\n",
    "    'Missing': df_feat[eng_feats].isna().sum(),\n",
    "    'Missing %': (df_feat[eng_feats].isna().sum() / len(df_feat) * 100).round(1)\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"=== Missing Values for Engineered Features ===\")\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 10. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# Drop rows without valid price target\nprint(f\"Original size: {len(df_feat):,} rows\")\ndf_model = df_feat[df_feat['_precio_num'].notna()].copy()\nprint(f\"After dropping missing prices: {len(df_model):,} rows\")\nprint(f\"Rows dropped: {len(df_feat) - len(df_model):,}\")\n\n# Save processed dataset - try parquet first, fall back to CSV\ntry:\n    output_path = DATA_DIR / 'db_features.parquet'\n    df_model.to_parquet(output_path, index=False)\n    print(f\"\\nSaved to: {output_path}\")\n    print(f\"File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\nexcept ImportError:\n    print(\"\\nNote: pyarrow not installed, saving as CSV instead\")\n    output_path = DATA_DIR / 'db_features.csv'\n    df_model.to_csv(output_path, index=False)\n    print(f\"\\nSaved to: {output_path}\")\n    print(f\"File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n\nprint(f\"Rows: {len(df_model):,}\")\nprint(f\"Columns: {len(df_model.columns)}\")\nprint(f\"Engineered features: {len([c for c in df_model.columns if c.startswith('_')])}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Feature Engineering Complete!\n",
    "\n",
    "**Key Improvements:**\n",
    "\n",
    "1. **Simplified CPU/GPU Parsing** - Extract normalized keys with brand, family, model, suffix\n",
    "2. **Two-Stage Matching** - Exact match first, then fuzzy match with similarity scores\n",
    "3. **Match Tracking** - `cpu_match_strategy` and `gpu_match_strategy` columns track how each row was matched\n",
    "4. **Integrated GPU Handling** - Correctly identifies and skips integrated graphics\n",
    "\n",
    "**Next Steps:**\n",
    "- Load processed dataset in modeling notebook\n",
    "- Build sklearn pipelines with imputation\n",
    "- Train and evaluate ML models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}