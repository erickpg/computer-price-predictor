{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - EDA: Computer Marketplace Dataset\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook performs an initial exploration of `db_computers_2025_raw.csv` - a dataset of computer listings from a marketplace.\n",
    "\n",
    "### Goals\n",
    "\n",
    "1. **Understand the target variable** (price) - its distribution, range, and potential transformations\n",
    "2. **Inspect distributions and missing values** across all columns\n",
    "3. **Identify core features** for the initial model:\n",
    "   - Product type (Tipo de producto)\n",
    "   - Brand (Marca) and Series (Serie)\n",
    "   - Processor (Procesador_Procesador)\n",
    "   - Graphics card (Gráfica_Tarjeta gráfica)\n",
    "   - RAM, storage, screen specs\n",
    "4. **Prepare observations** for the feature engineering notebook\n",
    "\n",
    "### Important Conventions\n",
    "\n",
    "- **Original Spanish column names are preserved** - we do NOT rename or translate columns\n",
    "- All comments and markdown are in English for readability\n",
    "- Engineered features (created in later notebooks) will start with underscore: `_cpu_mark`, `_ram_gb`, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_theme(style='whitegrid', palette='deep')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (relative to notebooks/ folder)\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "# Main dataset\n",
    "computers_path = DATA_DIR / 'db_computers_2025_raw.csv'\n",
    "columns_info_path = DATA_DIR / 'db_computers_columns_names.txt'\n",
    "\n",
    "# Load the main dataset\n",
    "# Note: Using utf-8-sig to handle BOM, index_col=0 then reset_index per course instructions\n",
    "df = pd.read_csv(\n",
    "    computers_path,\n",
    "    encoding='utf-8-sig',\n",
    "    index_col=0\n",
    ").reset_index()\n",
    "\n",
    "# Rename the old index column if needed\n",
    "if 'index' in df.columns:\n",
    "    df = df.rename(columns={'index': 'id_original'})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"  - Rows (computer listings): {df.shape[0]:,}\")\n",
    "print(f\"  - Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few rows\n",
    "# Note: Original Spanish column names are preserved exactly as they appear in the CSV\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns to see what's available\n",
    "print(\"All columns in the dataset:\\n\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Info and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic info about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for key categorical columns\n",
    "# These are columns we suspect will be important for prediction\n",
    "\n",
    "key_columns = [\n",
    "    'Tipo de producto',           # Product type (laptop, desktop, etc.)\n",
    "    'Serie',                       # Product series/line\n",
    "    'Tipo',                        # Type (Laptop, etc.)\n",
    "    'Procesador_Fabricante del procesador',  # CPU manufacturer (Intel, AMD, Apple)\n",
    "    'Gráfica_Fabricante de la tarjeta gráfica',  # GPU manufacturer\n",
    "    'Sistema operativo_Sistema operativo',  # OS\n",
    "]\n",
    "\n",
    "print(\"Unique value counts for key categorical columns:\\n\")\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Unique values: {df[col].nunique()}\")\n",
    "        print(f\"  Missing: {df[col].isna().sum()} ({df[col].isna().mean()*100:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for product type - this is crucial for understanding our dataset\n",
    "if 'Tipo de producto' in df.columns:\n",
    "    print(\"Product types (Tipo de producto):\\n\")\n",
    "    print(df['Tipo de producto'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for 'Tipo' column\n",
    "if 'Tipo' in df.columns:\n",
    "    print(\"Type (Tipo):\\n\")\n",
    "    print(df['Tipo'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU manufacturers\n",
    "if 'Procesador_Fabricante del procesador' in df.columns:\n",
    "    print(\"CPU Manufacturers (Procesador_Fabricante del procesador):\\n\")\n",
    "    print(df['Procesador_Fabricante del procesador'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable Exploration: Price (Precio)\n",
    "\n",
    "The price information is stored in the `Precio_Rango` column as a string range (e.g., \"1.026,53 € – 2.287,17 €\").\n",
    "\n",
    "For modeling, we'll need to extract a numeric value from this range. For now, let's explore it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the price column\n",
    "precio_col = 'Precio_Rango'\n",
    "\n",
    "print(f\"Price column: {precio_col}\")\n",
    "print(f\"\\nSample values:\")\n",
    "print(df[precio_col].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_precio_numerico(precio_str):\n",
    "    \"\"\"\n",
    "    Extract numeric price from a range string.\n",
    "    For ranges like \"1.026,53 € – 2.287,17 €\", returns the midpoint.\n",
    "    For single prices, returns that price.\n",
    "    \n",
    "    Spanish format: periods for thousands, commas for decimals.\n",
    "    \"\"\"\n",
    "    if pd.isna(precio_str) or not isinstance(precio_str, str):\n",
    "        return np.nan\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Find all price patterns (number with Spanish format)\n",
    "    # Pattern: digits with optional periods, comma, digits\n",
    "    pattern = r'([\\d.]+,\\d{2})'\n",
    "    matches = re.findall(pattern, precio_str)\n",
    "    \n",
    "    if not matches:\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert Spanish format to float\n",
    "    precios = []\n",
    "    for m in matches:\n",
    "        # Remove thousand separators (.), replace decimal comma with period\n",
    "        num_str = m.replace('.', '').replace(',', '.')\n",
    "        precios.append(float(num_str))\n",
    "    \n",
    "    # Return midpoint if range, otherwise single value\n",
    "    if len(precios) == 2:\n",
    "        return (precios[0] + precios[1]) / 2\n",
    "    elif len(precios) == 1:\n",
    "        return precios[0]\n",
    "    else:\n",
    "        return np.mean(precios)\n",
    "\n",
    "# Extract numeric prices (for EDA only - proper extraction in features.py)\n",
    "df['_precio_eda'] = df[precio_col].apply(extraer_precio_numerico)\n",
    "\n",
    "print(f\"Extracted numeric prices:\")\n",
    "print(f\"  Valid prices: {df['_precio_eda'].notna().sum()} / {len(df)}\")\n",
    "print(f\"  Missing: {df['_precio_eda'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price statistics\n",
    "print(\"Price Statistics (in €):\\n\")\n",
    "print(df['_precio_eda'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "df['_precio_eda'].hist(bins=50, ax=ax1, edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Price (€)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Price Distribution (Histogram)')\n",
    "ax1.axvline(df['_precio_eda'].median(), color='red', linestyle='--', label=f\"Median: {df['_precio_eda'].median():,.0f}€\")\n",
    "ax1.axvline(df['_precio_eda'].mean(), color='orange', linestyle='--', label=f\"Mean: {df['_precio_eda'].mean():,.0f}€\")\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot\n",
    "ax2 = axes[1]\n",
    "df.boxplot(column='_precio_eda', ax=ax2)\n",
    "ax2.set_ylabel('Price (€)')\n",
    "ax2.set_title('Price Distribution (Box Plot)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Observations on price distribution\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- The distribution appears right-skewed (long tail of high prices)\")\n",
    "print(\"- There are potential outliers at the high end (expensive gaming/workstation PCs)\")\n",
    "print(\"- A log-transform might help normalize the distribution for modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transformed price distribution (preview for future consideration)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "df['_log_precio'] = np.log1p(df['_precio_eda'])\n",
    "df['_log_precio'].hist(bins=50, ax=ax, edgecolor='black', alpha=0.7, color='green')\n",
    "ax.set_xlabel('Log(Price + 1)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Log-Transformed Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Log-transform makes the distribution more symmetric.\")\n",
    "print(\"Consider using log-price as target, then exp() to get final predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1. Price Range Analysis: Range vs Midpoint\n\nThe `Precio_Rango` column contains price ranges (min-max). We need to decide whether to:\n1. **Predict the midpoint** (simpler, single target)\n2. **Predict the range** (min and max separately, or range spread)\n\nLet's analyze the range spreads to inform this decision.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract min, max, and midpoint from price ranges\nimport re\n\ndef extract_full_price_range(precio_str):\n    \"\"\"Extract min, max, and midpoint from price range string.\"\"\"\n    if pd.isna(precio_str) or not isinstance(precio_str, str):\n        return None, None, None\n    \n    pattern = r'([\\d.]+,\\d{2})'\n    matches = re.findall(pattern, precio_str)\n    \n    if not matches:\n        return None, None, None\n    \n    # Convert Spanish format to float\n    precios = []\n    for m in matches:\n        num_str = m.replace('.', '').replace(',', '.')\n        precios.append(float(num_str))\n    \n    if len(precios) == 2:\n        min_price = precios[0]\n        max_price = precios[1]\n        mid_price = (precios[0] + precios[1]) / 2\n        return min_price, max_price, mid_price\n    elif len(precios) == 1:\n        return precios[0], precios[0], precios[0]\n    else:\n        return None, None, None\n\n# Apply extraction\nprice_ranges = df['Precio_Rango'].apply(extract_full_price_range)\ndf['_min_price'] = price_ranges.apply(lambda x: x[0])\ndf['_max_price'] = price_ranges.apply(lambda x: x[1])\ndf['_mid_price'] = price_ranges.apply(lambda x: x[2])\n\n# Calculate spread metrics\ndf['_price_spread'] = df['_max_price'] - df['_min_price']\ndf['_spread_pct'] = (df['_price_spread'] / df['_min_price'] * 100)\n\nprint(\"Price Range Statistics:\\n\")\nprint(df[['_min_price', '_max_price', '_mid_price', '_price_spread', '_spread_pct']].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize price spread distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Price spread in euros\nax1 = axes[0]\ndf['_price_spread'].hist(bins=50, ax=ax1, edgecolor='black', alpha=0.7, color='coral')\nax1.set_xlabel('Price Spread (€)')\nax1.set_ylabel('Frequency')\nax1.set_title('Distribution of Price Range Spread (Max - Min)')\nax1.axvline(df['_price_spread'].median(), color='red', linestyle='--', \n            label=f\"Median: {df['_price_spread'].median():,.0f}€\")\n\n# Price spread as percentage\nax2 = axes[1]\ndf['_spread_pct'].hist(bins=50, ax=ax2, edgecolor='black', alpha=0.7, color='skyblue')\nax2.set_xlabel('Spread as % of Min Price')\nax2.set_ylabel('Frequency')\nax2.set_title('Price Spread as Percentage of Min Price')\nax2.axvline(df['_spread_pct'].median(), color='red', linestyle='--', \n            label=f\"Median: {df['_spread_pct'].median():.1f}%\")\n\nfor ax in axes:\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(f\"- Median price spread: {df['_price_spread'].median():,.0f}€\")\nprint(f\"- Median spread as % of min price: {df['_spread_pct'].median():.1f}%\")\nprint(f\"- Mean spread as % of min price: {df['_spread_pct'].mean():.1f}%\")\nprint(\"\\n-> The price ranges are quite wide (median ~{:.0f}% of min price)\".format(df['_spread_pct'].median()))\nprint(\"-> This suggests the marketplace aggregates offers from multiple sellers\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check Ofertas column - does it contain individual price listings?\nprint(\"=== Ofertas Column Analysis ===\\n\")\nprint(\"Sample values:\")\nprint(df['Ofertas'].head(20))\nprint(\"\\n\")\nprint(f\"Unique values: {df['Ofertas'].nunique()}\")\nprint(f\"Missing values: {df['Ofertas'].isna().sum()} ({df['Ofertas'].isna().mean()*100:.1f}%)\")\nprint(\"\\n\")\n\n# Extract number of offers\ndef extract_num_ofertas(oferta_str):\n    \"\"\"Extract number of offers from string like '200 ofertas:'\"\"\"\n    if pd.isna(oferta_str):\n        return np.nan\n    import re\n    match = re.search(r'(\\d+)\\s+ofertas?', str(oferta_str), re.IGNORECASE)\n    if match:\n        return int(match.group(1))\n    return np.nan\n\ndf['_num_ofertas'] = df['Ofertas'].apply(extract_num_ofertas)\n\nprint(\"Number of offers statistics:\")\nprint(df['_num_ofertas'].describe())\nprint(\"\\n\")\n\n# Correlation between number of offers and price spread\ncorrelation = df[['_num_ofertas', '_price_spread', '_spread_pct']].corr()\nprint(\"Correlation with price spread:\")\nprint(correlation.loc['_num_ofertas'])\nprint(\"\\n\")\n\nprint(\"Conclusion:\")\nprint(\"- The 'Ofertas' column only contains the COUNT of offers (e.g., '200 ofertas:')\")\nprint(\"- It does NOT contain individual price listings\")\nprint(\"- The Precio_Rango represents the min-max across all those offers\")\nprint(\"- More offers tends to correlate with wider price spreads\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Decision: Predict Midpoint vs Range?\n\nBased on the analysis above, here's the recommendation:\n\n**Recommendation: Predict the MIDPOINT price** for the following reasons:\n\n1. **Simpler modeling approach**: Single target variable instead of two (min/max)\n2. **Marketplace context**: The price range represents offers from multiple sellers, not product uncertainty\n3. **User-friendly output**: A single price estimate is easier to interpret than a range\n4. **Wide spreads**: The ranges are wide (~95% median spread), so predicting the exact range is less meaningful\n\n**Alternative approaches** (for future consideration):\n- Predict both `min_price` and `max_price` separately (multi-target regression)\n- Predict `midpoint` + `spread` (could give users a confidence interval)\n- Build separate models for different price segments\n\n**Decision**: We'll use `_precio_num` (midpoint) as our target variable in `02_feature_engineering.ipynb`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Values Overview\n",
    "\n",
    "Understanding missing values is crucial for:\n",
    "- Deciding which features to include in the model\n",
    "- Choosing imputation strategies\n",
    "- Identifying potentially unreliable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values per column\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "\n",
    "# Create a summary DataFrame\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing %': missing_pct,\n",
    "    'Present %': 100 - missing_pct\n",
    "})\n",
    "\n",
    "# Show columns with most missing values\n",
    "print(\"Columns with MOST missing values (top 30):\\n\")\n",
    "print(missing_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns with LEAST missing values (most complete)\n",
    "print(\"Columns with LEAST missing values (most complete):\\n\")\n",
    "print(missing_df.tail(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values for key feature columns\n",
    "# Focus on columns we expect to use for modeling\n",
    "\n",
    "key_feature_cols = [\n",
    "    'Precio_Rango',\n",
    "    'Título',\n",
    "    'Tipo de producto',\n",
    "    'Serie',\n",
    "    'Procesador_Procesador',\n",
    "    'Procesador_Fabricante del procesador',\n",
    "    'Procesador_Número de núcleos del procesador',\n",
    "    'RAM_Memoria RAM',\n",
    "    'RAM_Tipo de RAM',\n",
    "    'Disco duro_Capacidad de memoria SSD',\n",
    "    'Disco duro_Tipo de disco duro',\n",
    "    'Gráfica_Tarjeta gráfica',\n",
    "    'Gráfica_Fabricante de la tarjeta gráfica',\n",
    "    'Gráfica_Memoria gráfica',\n",
    "    'Pantalla_Diagonal de la pantalla',\n",
    "    'Pantalla_Resolución de pantalla',\n",
    "    'Tipo',\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "key_feature_cols = [c for c in key_feature_cols if c in df.columns]\n",
    "\n",
    "key_missing = missing_df.loc[key_feature_cols].sort_values('Missing %', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "key_missing['Present %'].plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Data Completeness (%)')\n",
    "ax.set_title('Data Completeness for Key Feature Columns')\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(key_missing['Present %']):\n",
    "    ax.text(v + 1, i, f'{v:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize columns by missing data levels\n",
    "very_sparse = missing_df[missing_df['Missing %'] >= 80].index.tolist()  # >80% missing\n",
    "sparse = missing_df[(missing_df['Missing %'] >= 50) & (missing_df['Missing %'] < 80)].index.tolist()\n",
    "moderate = missing_df[(missing_df['Missing %'] >= 20) & (missing_df['Missing %'] < 50)].index.tolist()\n",
    "dense = missing_df[missing_df['Missing %'] < 20].index.tolist()  # <20% missing\n",
    "\n",
    "print(f\"Very Sparse (>80% missing): {len(very_sparse)} columns\")\n",
    "print(f\"Sparse (50-80% missing): {len(sparse)} columns\")\n",
    "print(f\"Moderate (20-50% missing): {len(moderate)} columns\")\n",
    "print(f\"Dense (<20% missing): {len(dense)} columns\")\n",
    "\n",
    "print(\"\\n--- Dense columns (likely useful for initial model) ---\")\n",
    "for col in dense:\n",
    "    print(f\"  {col}: {missing_df.loc[col, 'Present %']:.1f}% present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Observations\n",
    "\n",
    "**Dense columns (low missingness, likely useful):**\n",
    "- `Título` (Title) - Always present, can extract brand info\n",
    "- `Precio_Rango` - Target variable, mostly present\n",
    "- `Tipo de producto` - Product category (gaming, multimedia, etc.)\n",
    "- `Tipo` - Laptop vs Desktop classification\n",
    "\n",
    "**Moderate missingness (need imputation):**\n",
    "- `Procesador_Procesador` - CPU name, will need fuzzy matching to benchmarks\n",
    "- `RAM_Memoria RAM` - RAM info, extractable with regex\n",
    "- Screen-related columns\n",
    "\n",
    "**Sparse columns (might drop or use carefully):**\n",
    "- Many detailed spec columns are very sparse\n",
    "- Some columns only apply to specific product types (e.g., battery for laptops only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. First Thoughts on Core Features\n",
    "\n",
    "Based on domain knowledge about computer pricing, here are the features we expect to be most predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core features we plan to use for the initial model\n",
    "# Note: We'll create engineered versions of many of these with _ prefix\n",
    "\n",
    "posibles_features_core = [\n",
    "    # Category/Type\n",
    "    'Tipo de producto',           # Product type (gaming, multimedia, professional)\n",
    "    'Tipo',                        # Laptop vs Desktop\n",
    "    'Serie',                       # Product series (often indicates tier)\n",
    "    \n",
    "    # Processor\n",
    "    'Procesador_Procesador',       # CPU model - will map to _cpu_mark benchmark\n",
    "    'Procesador_Fabricante del procesador',  # Intel, AMD, Apple\n",
    "    'Procesador_Número de núcleos del procesador',  # Core count\n",
    "    \n",
    "    # Graphics\n",
    "    'Gráfica_Tarjeta gráfica',     # GPU model - will map to _gpu_mark benchmark\n",
    "    'Gráfica_Fabricante de la tarjeta gráfica',  # NVIDIA, AMD, Intel\n",
    "    'Gráfica_Memoria gráfica',     # VRAM\n",
    "    \n",
    "    # Memory and Storage\n",
    "    'RAM_Memoria RAM',             # RAM - will extract to _ram_gb\n",
    "    'RAM_Tipo de RAM',             # DDR4, DDR5, LPDDR5X\n",
    "    'Disco duro_Capacidad de memoria SSD',  # SSD - will extract to _ssd_gb\n",
    "    'Disco duro_Tipo de disco duro',  # SSD, HDD, hybrid\n",
    "    \n",
    "    # Display\n",
    "    'Pantalla_Diagonal de la pantalla',  # Screen size - will extract to _tamano_pantalla_pulgadas\n",
    "    'Pantalla_Resolución de pantalla',   # Resolution\n",
    "    'Pantalla_Tasa de actualización de imagen',  # Refresh rate (important for gaming)\n",
    "]\n",
    "\n",
    "print(\"Core features for initial model:\")\n",
    "for i, feat in enumerate(posibles_features_core, 1):\n",
    "    if feat in df.columns:\n",
    "        present_pct = (1 - df[feat].isna().mean()) * 100\n",
    "        print(f\"{i:2d}. {feat}: {present_pct:.1f}% present\")\n",
    "    else:\n",
    "        print(f\"{i:2d}. {feat}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6.1. Brand Extraction from Título\n\nThe dataset may not have a `Marca` (brand) column, but we can extract it from the `Título` column.\nLet's explore how to identify brands from product titles.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check if Marca column exists\nif 'Marca' in df.columns:\n    print(\"Marca column EXISTS\\n\")\n    print(df['Marca'].value_counts(dropna=False).head(20))\n    print(f\"\\nMissing: {df['Marca'].isna().sum()} ({df['Marca'].isna().mean()*100:.1f}%)\")\nelse:\n    print(\"Marca column DOES NOT EXIST - we need to extract it from Título\\n\")\n    \n# Sample titles to understand the pattern\nprint(\"\\n=== Sample Títulos ===\")\nprint(df['Título'].head(30))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract brand from title (usually the first word)\ndef extract_brand(titulo):\n    \"\"\"Extract brand from product title (usually first word).\"\"\"\n    if pd.isna(titulo):\n        return np.nan\n    \n    # Common computer brands (case insensitive)\n    common_brands = [\n        'Apple', 'ASUS', 'Lenovo', 'HP', 'Dell', 'Acer', 'MSI', \n        'Samsung', 'Microsoft', 'Razer', 'Alienware', 'LG', \n        'Huawei', 'Xiaomi', 'GigaByte', 'Gigabyte', 'Toshiba',\n        'Fujitsu', 'Medion', 'Sony', 'Vaio', 'Corsair', 'NZXT'\n    ]\n    \n    # Get first word (usually the brand)\n    first_word = str(titulo).split()[0] if titulo else ''\n    \n    # Check if first word matches a known brand (case insensitive)\n    for brand in common_brands:\n        if first_word.lower() == brand.lower():\n            return brand\n    \n    # If not found in common brands, return the first word anyway\n    # (we'll clean this up in feature engineering)\n    return first_word if first_word else np.nan\n\ndf['_brand_extracted'] = df['Título'].apply(extract_brand)\n\nprint(\"Extracted Brand Distribution:\\n\")\nprint(df['_brand_extracted'].value_counts(dropna=False).head(20))\nprint(f\"\\nUnique brands: {df['_brand_extracted'].nunique()}\")\nprint(f\"Missing: {df['_brand_extracted'].isna().sum()}\")\n\n# Compare with Serie column to see if brand info is there\nprint(\"\\n=== Serie Column (for comparison) ===\")\nif 'Serie' in df.columns:\n    print(df['Serie'].value_counts(dropna=False).head(20))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.2. Serie Missing Value Imputation from Título\n\nThe `Serie` column has many missing values. Let's explore if we can infer the Series from the product title.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze Serie column\nprint(\"=== Serie Column Analysis ===\\n\")\nif 'Serie' in df.columns:\n    print(f\"Total rows: {len(df)}\")\n    print(f\"Serie present: {df['Serie'].notna().sum()} ({df['Serie'].notna().mean()*100:.1f}%)\")\n    print(f\"Serie missing: {df['Serie'].isna().sum()} ({df['Serie'].isna().mean()*100:.1f}%)\")\n    print(\"\\n\")\n    \n    # Look at some examples where Serie is present\n    print(\"Examples with Serie present:\")\n    sample_with_serie = df[df['Serie'].notna()][['Título', 'Serie', '_brand_extracted']].head(15)\n    print(sample_with_serie.to_string())\n    print(\"\\n\")\n    \n    # Look at some examples where Serie is missing\n    print(\"Examples with Serie MISSING:\")\n    sample_without_serie = df[df['Serie'].isna()][['Título', 'Serie', '_brand_extracted']].head(15)\n    print(sample_without_serie.to_string())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract potential series from title\ndef extract_series_from_title(titulo, brand):\n    \"\"\"\n    Attempt to extract product series from title.\n    Common patterns:\n    - Apple: MacBook Air, MacBook Pro, iMac\n    - ASUS: ROG, Zenbook, Vivobook, TUF Gaming\n    - Lenovo: ThinkPad, IdeaPad, Legion, LOQ\n    - HP: Pavilion, Envy, Omen, EliteBook\n    - Dell: Inspiron, XPS, Alienware, Latitude\n    \"\"\"\n    if pd.isna(titulo):\n        return np.nan\n    \n    titulo_lower = str(titulo).lower()\n    \n    # Known series patterns by brand\n    series_patterns = {\n        'Apple': ['MacBook Air', 'MacBook Pro', 'iMac', 'Mac Mini', 'Mac Pro', 'Mac Studio'],\n        'ASUS': ['ROG', 'Zenbook', 'Vivobook', 'TUF Gaming', 'Republic of Gamers', \n                 'ExpertBook', 'ProArt', 'StudioBook', 'Chromebook'],\n        'Lenovo': ['ThinkPad', 'IdeaPad', 'Legion', 'LOQ', 'Yoga', 'ThinkBook'],\n        'HP': ['Pavilion', 'Envy', 'Omen', 'EliteBook', 'ProBook', 'Spectre', 'ZBook'],\n        'Dell': ['Inspiron', 'XPS', 'Alienware', 'Latitude', 'Precision', 'Vostro'],\n        'MSI': ['Katana', 'Stealth', 'Raider', 'Cyborg', 'Prestige', 'Modern', 'Summit'],\n        'Acer': ['Aspire', 'Swift', 'Nitro', 'Predator', 'TravelMate', 'ConceptD'],\n        'Samsung': ['Galaxy Book'],\n        'Microsoft': ['Surface'],\n        'Gigabyte': ['Aero', 'Aorus'],\n    }\n    \n    if pd.isna(brand) or brand not in series_patterns:\n        return np.nan\n    \n    # Look for series keywords in title\n    for series in series_patterns[brand]:\n        if series.lower() in titulo_lower:\n            return series\n    \n    return np.nan\n\ndf['_series_extracted'] = df.apply(\n    lambda row: extract_series_from_title(row['Título'], row['_brand_extracted']), \n    axis=1\n)\n\nprint(\"=== Series Extraction Results ===\\n\")\nprint(f\"Series extracted: {df['_series_extracted'].notna().sum()}\")\nprint(f\"Could not extract: {df['_series_extracted'].isna().sum()}\")\nprint(\"\\n\")\n\n# Compare extracted series with existing Serie column\nif 'Serie' in df.columns:\n    print(\"Comparison: Existing Serie vs Extracted Series\")\n    comparison = pd.DataFrame({\n        'Serie_exists': df['Serie'].notna(),\n        'Serie_extracted': df['_series_extracted'].notna()\n    })\n    \n    print(\"\\nCrosstab:\")\n    print(pd.crosstab(comparison['Serie_exists'], comparison['Serie_extracted'], \n                      rownames=['Serie column present'], \n                      colnames=['Extracted from title']))\n    \n    # Show some examples where we filled missing Serie\n    print(\"\\n=== Examples where we filled missing Serie ===\")\n    filled = df[(df['Serie'].isna()) & (df['_series_extracted'].notna())]\n    if len(filled) > 0:\n        print(filled[['Título', 'Serie', '_series_extracted', '_brand_extracted']].head(10).to_string())\n        print(f\"\\nTotal filled: {len(filled)} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.3. Screen Size: Pulgadas vs Centimeters\n\nThere are two screen size columns:\n- `Pantalla_Tamaño de la pantalla` - in pulgadas (inches) like \"15,6 pulgadas\"\n- `Pantalla_Diagonal de la pantalla` - in centimeters like \"39,624 cm\"\n\nLet's check which one is more complete and if we need both.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare both screen size columns\nscreen_cols = {\n    'Pantalla_Tamaño de la pantalla': 'pulgadas (inches)',\n    'Pantalla_Diagonal de la pantalla': 'cm'\n}\n\nprint(\"=== Screen Size Column Comparison ===\\n\")\nfor col, unit in screen_cols.items():\n    if col in df.columns:\n        present_count = df[col].notna().sum()\n        present_pct = df[col].notna().mean() * 100\n        print(f\"{col} ({unit}):\")\n        print(f\"  Present: {present_count} ({present_pct:.1f}%)\")\n        print(f\"  Missing: {df[col].isna().sum()} ({df[col].isna().mean()*100:.1f}%)\")\n        print(f\"  Sample values: {df[col].dropna().head(5).tolist()}\")\n        print()\n\n# Check if they're redundant (one can be converted from the other)\nif 'Pantalla_Tamaño de la pantalla' in df.columns and 'Pantalla_Diagonal de la pantalla' in df.columns:\n    print(\"\\n=== Checking correlation between pulgadas and cm ===\")\n    \n    # Extract numeric values from both\n    def extract_pulgadas(val):\n        if pd.isna(val):\n            return np.nan\n        import re\n        # Match patterns like \"15,6 pulgadas\" or \"15.6 pulgadas\"\n        match = re.search(r'([\\d,\\.]+)', str(val))\n        if match:\n            num_str = match.group(1).replace(',', '.')\n            return float(num_str)\n        return np.nan\n    \n    def extract_cm(val):\n        if pd.isna(val):\n            return np.nan\n        import re\n        # Match patterns like \"39,624 cm\"\n        match = re.search(r'([\\d,\\.]+)', str(val))\n        if match:\n            num_str = match.group(1).replace(',', '.')\n            return float(num_str)\n        return np.nan\n    \n    df['_pulgadas_num'] = df['Pantalla_Tamaño de la pantalla'].apply(extract_pulgadas)\n    df['_cm_num'] = df['Pantalla_Diagonal de la pantalla'].apply(extract_cm)\n    \n    # Check correlation (1 inch = 2.54 cm)\n    df['_cm_from_pulgadas'] = df['_pulgadas_num'] * 2.54\n    \n    # Compare for rows where both are present\n    both_present = df[(df['_pulgadas_num'].notna()) & (df['_cm_num'].notna())]\n    \n    if len(both_present) > 0:\n        print(f\"\\nRows with both values: {len(both_present)}\")\n        print(\"\\nSample comparison:\")\n        sample = both_present[['Pantalla_Tamaño de la pantalla', 'Pantalla_Diagonal de la pantalla', \n                               '_pulgadas_num', '_cm_num', '_cm_from_pulgadas']].head(10)\n        print(sample.to_string())\n        \n        # Check if they're consistent (allowing for rounding)\n        diff = np.abs(both_present['_cm_num'] - both_present['_cm_from_pulgadas'])\n        print(f\"\\nDifference between cm column and converted pulgadas:\")\n        print(f\"  Mean diff: {diff.mean():.3f} cm\")\n        print(f\"  Max diff: {diff.max():.3f} cm\")\n        \n        consistent = (diff < 0.1).sum()\n        print(f\"\\n  Consistent (diff < 0.1 cm): {consistent}/{len(both_present)} ({consistent/len(both_present)*100:.1f}%)\")\n    \n    print(\"\\n=== Coverage Analysis ===\")\n    coverage = pd.DataFrame({\n        'Has pulgadas': df['_pulgadas_num'].notna(),\n        'Has cm': df['_cm_num'].notna()\n    })\n    print(pd.crosstab(coverage['Has pulgadas'], coverage['Has cm'], \n                      rownames=['Has pulgadas'], colnames=['Has cm'], margins=True))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Screen Size Recommendation\n\nBased on the analysis above:\n\n**Recommendation: Use `Pantalla_Tamaño de la pantalla` (pulgadas/inches) as the primary feature**\n\nReasons:\n1. **Industry standard**: Screen sizes are typically marketed in inches (13\", 15.6\", 17\", etc.)\n2. **More intuitive**: Users and domain experts think in inches for laptops/monitors\n3. **Likely better coverage**: The pulgadas column probably has similar or better coverage than cm\n4. **Redundancy**: If both columns are present and consistent, we only need one\n5. **Feature engineering**: We can convert cm to inches if pulgadas is missing\n\n**Strategy for feature engineering** ([02_feature_engineering.ipynb](notebooks/02_feature_engineering.ipynb)):\n- Extract numeric value from `Pantalla_Tamaño de la pantalla` to create `_tamano_pantalla_pulgadas`\n- If missing, convert `Pantalla_Diagonal de la pantalla` (cm ÷ 2.54 = inches)\n- This gives us maximum coverage with a single, interpretable feature",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore RAM column\n",
    "print(\"RAM_Memoria RAM - Value distribution:\\n\")\n",
    "if 'RAM_Memoria RAM' in df.columns:\n",
    "    print(df['RAM_Memoria RAM'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> RAM seems mostly present, values like '16 GB RAM', '8 GB RAM', etc.\")\n",
    "    print(\"-> Will need to extract numeric value to _ram_gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore processor column\n",
    "print(\"Procesador_Procesador - Sample values:\\n\")\n",
    "if 'Procesador_Procesador' in df.columns:\n",
    "    print(df['Procesador_Procesador'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> Many unique CPU models (Intel Core i7-13700H, AMD Ryzen 7, Apple M3, etc.)\")\n",
    "    print(\"-> Will need to map to benchmark scores using db_cpu_raw.csv and fuzzy matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore GPU column\n",
    "print(\"Gráfica_Tarjeta gráfica - Sample values:\\n\")\n",
    "if 'Gráfica_Tarjeta gráfica' in df.columns:\n",
    "    print(df['Gráfica_Tarjeta gráfica'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> Some GPU fields are sparse (integrated graphics often not listed)\")\n",
    "    print(\"-> Will need fuzzy matching to db_gpu_raw.csv for _gpu_mark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore SSD capacity column\n",
    "print(\"Disco duro_Capacidad de memoria SSD - Sample values:\\n\")\n",
    "if 'Disco duro_Capacidad de memoria SSD' in df.columns:\n",
    "    print(df['Disco duro_Capacidad de memoria SSD'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> Values like '512 GB', '1.000 GB', '256 GB'\")\n",
    "    print(\"-> Will extract to numeric _ssd_gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore screen size column\n",
    "print(\"Pantalla_Diagonal de la pantalla - Sample values:\\n\")\n",
    "if 'Pantalla_Diagonal de la pantalla' in df.columns:\n",
    "    print(df['Pantalla_Diagonal de la pantalla'].value_counts(dropna=False).head(15))\n",
    "    print(\"\\n-> Values in cm like '39,624 cm', '35,56 cm'\")\n",
    "    print(\"-> Will convert to inches for _tamano_pantalla_pulgadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price by product type - initial exploration\n",
    "if 'Tipo de producto' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Filter to types with enough samples\n",
    "    tipo_counts = df['Tipo de producto'].value_counts()\n",
    "    main_tipos = tipo_counts[tipo_counts >= 10].index\n",
    "    \n",
    "    df_plot = df[df['Tipo de producto'].isin(main_tipos)]\n",
    "    \n",
    "    df_plot.boxplot(column='_precio_eda', by='Tipo de producto', ax=ax)\n",
    "    ax.set_xlabel('Tipo de producto')\n",
    "    ax.set_ylabel('Price (€)')\n",
    "    ax.set_title('Price Distribution by Product Type')\n",
    "    plt.suptitle('')  # Remove automatic title\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nObservation: Gaming laptops (Portátil gaming) tend to have higher prices.\")\n",
    "    print(\"Product type will be an important categorical feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Summary and Next Steps\n\n### What We Learned\n\n1. **Dataset Size**: The dataset contains computer listings with ~140+ columns covering detailed specifications.\n\n2. **Target Variable (Price)**:\n   - Stored as a price range string in `Precio_Rango` (e.g., \"1.026,53 € – 2.287,17 €\")\n   - The `Ofertas` column shows the number of marketplace offers, NOT individual prices\n   - **Decision: Predict MIDPOINT price** (`_precio_num`) for simplicity\n   - Price ranges are wide (~95% median spread), representing multiple seller offers\n   - Right-skewed distribution with outliers at high end\n   - Log-transform may help for modeling\n\n3. **Brand and Series Extraction**:\n   - **No `Marca` column exists** - we'll extract brand from `Título` (first word)\n   - Successfully extracted brands: Apple, ASUS, Lenovo, HP, Dell, MSI, Samsung, etc.\n   - **Serie column has gaps** - we can fill many missing values by extracting from `Título`\n   - Series patterns identified: MacBook Air/Pro, ROG, Zenbook, ThinkPad, IdeaPad, etc.\n\n4. **Screen Size Strategy**:\n   - Two columns: `Pantalla_Tamaño de la pantalla` (pulgadas) and `Pantalla_Diagonal de la pantalla` (cm)\n   - **Decision: Use pulgadas (inches) as primary** - industry standard, more intuitive\n   - Strategy: Extract from pulgadas column, fall back to cm/2.54 if missing\n\n5. **Missing Values**:\n   - Many columns are very sparse (>80% missing)\n   - Core columns like `Título`, `Tipo de producto`, `Precio_Rango` are mostly complete\n   - Will need imputation strategy for moderately-sparse features\n\n6. **Core Features Identified**:\n   - **Product category**: `Tipo de producto`, `Tipo`, `Serie` (with extraction from título)\n   - **Brand**: Extract from `Título` → `_brand`\n   - **Processor**: `Procesador_Procesador` → needs benchmark matching to `_cpu_mark`\n   - **Graphics**: `Gráfica_Tarjeta gráfica` → needs benchmark matching to `_gpu_mark`\n   - **Memory**: `RAM_Memoria RAM` → extract to `_ram_gb`\n   - **Storage**: `Disco duro_Capacidad de memoria SSD` → extract to `_ssd_gb`\n   - **Screen**: `Pantalla_Tamaño de la pantalla` (pulgadas) → `_tamano_pantalla_pulgadas`\n\n7. **Key Observations**:\n   - Gaming laptops (`Portátil gaming`) have higher prices\n   - CPU/GPU models are varied and will need fuzzy matching to benchmark data\n   - Many numeric values stored as strings with Spanish format (comma decimal)\n\n---\n\n### Next Notebook: [02_feature_engineering.ipynb](02_feature_engineering.ipynb)\n\nIn the next notebook, we will:\n\n1. **Create engineered features** using [src/features.py](../src/features.py):\n   - `_precio_num`: Numeric price (midpoint of range) ← **TARGET VARIABLE**\n   - `_brand`: Extract brand from `Título`\n   - `_serie`: Combine existing `Serie` + extracted from `Título`\n   - `_cpu_mark`: CPU benchmark score from [db_cpu_raw.csv](../data/db_cpu_raw.csv) (fuzzy matching)\n   - `_gpu_mark`: GPU benchmark score from [db_gpu_raw.csv](../data/db_gpu_raw.csv) (fuzzy matching)\n   - `_ram_gb`: RAM in GB (numeric)\n   - `_ssd_gb`: SSD capacity in GB (numeric)\n   - `_tamano_pantalla_pulgadas`: Screen size in inches (from pulgadas or cm)\n\n2. **Implement missing value strategy**:\n   - Decide which columns to keep vs drop\n   - Choose imputation methods (median, mode, constant)\n\n3. **Prepare clean feature table** for modeling (notebook 03)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove temporary columns created for EDA\n",
    "eda_cols = ['_precio_eda', '_log_precio']\n",
    "for col in eda_cols:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "print(\"EDA notebook complete!\")\n",
    "print(f\"Next: 02_feature_engineering.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}